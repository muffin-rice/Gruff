{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import gym, gym.spaces\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from python.BridgeNetwork import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class BridgeCritic(BridgeSupervised): # learns the Value of a given state (discounted total reward)\n",
    "class BridgeCritic(BridgeBase): # learns the Value of a given state (discounted total reward)\n",
    "    def __init__(self, file_dir, input_dim = 571, inner_dim = 256, num_blocks=2):\n",
    "        #super().__init__(input_dim, inner_dim, num_blocks)\n",
    "        super().__init__()\n",
    "        self.load_state_dict(torch.load(file_dir)['state_dict'])\n",
    "        self.critic_out = nn.Linear(NUM_ACTIONS, 1)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.metrics_fn = lambda yhat,y: {'r2' :1 - ((y - yhat)^2).sum()/((y - y.mean())^2).sum() }\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Outputs single value\n",
    "        '''\n",
    "        x = self.forward_half(x)\n",
    "        x = self.critic_out(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['observation'], batch['labels']\n",
    "        yhat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        metrics = self.metrics_fn(yhat, y)\n",
    "\n",
    "        return {'loss' : loss, **metrics}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['observation'], batch['labels']\n",
    "        yhat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        metrics = self.metrics_fn(yhat.detach(), y.detach())\n",
    "\n",
    "        return {'loss' : loss, **metrics}\n",
    "\n",
    "#class BridgeActor(BridgeSupervised): # learns the optimal policy fn ( optimal f(action, state) = probability(action|state) )\n",
    "class BridgeActor(BridgeBase): # learns the optimal policy fn ( optimal f(action, state) = probability(action|state) )\n",
    "    def __init__(self, file_dir, input_dim = 571, inner_dim = 256, num_blocks=2):\n",
    "        #super().__init__(input_dim, inner_dim, num_blocks)\n",
    "        super().__init__()\n",
    "        self.load_state_dict(torch.load(file_dir)['state_dict'])\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['observation'], batch['labels']\n",
    "        yhat = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        metrics = self.metrics_fn(yhat, y)\n",
    "\n",
    "        return {'loss' : loss, **metrics}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, action, advantage = batch['observation'], batch['action'], batch['advantage']\n",
    "        yhat = self.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        metrics = self.metrics_fn(yhat.detach(), y.detach())\n",
    "\n",
    "        return {'loss' : loss, **metrics}\n",
    "\n",
    "\n",
    "class BridgeActorCritic(pl.LightningModule):\n",
    "    def __init__(self, file_dir, input_dim = 571, inner_dim = 256, num_blocks=2):\n",
    "        super().__init__()\n",
    "        self.actor = BridgeActor(file_dir, input_dim, inner_dim, num_blocks)\n",
    "        self.critic = BridgeCritic(file_dir, input_dim, inner_dim, num_blocks)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-4)\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Outputs value, policy_distribution\n",
    "        '''\n",
    "        value = self.critic.forward(x)\n",
    "        policy_dist = self.actor.forward(x)\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = pyspiel.load_game('bridge(use_double_dummy_result=true)')\n",
    "\n",
    "class BridgeEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BridgeEnv, self).__init__()    # Define action and observation space\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(38,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(571,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.generate_random_game()\n",
    "        return np.array(self.state.observation_tensor())\n",
    "\n",
    "    def step(self, action):\n",
    "        action = self.pick_action(action)\n",
    "\n",
    "        self.state.apply_action(action+52)\n",
    "        \n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action)\n",
    "        \n",
    "        # random opposing team\n",
    "        self.state.apply_action(random.choice(self.state.legal_actions()))\n",
    "\n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action)\n",
    "\n",
    "        return self.calculate_default_reward(action)\n",
    "\n",
    "    def calculate_default_reward(self, action):\n",
    "        obs = np.array(self.state.observation_tensor())\n",
    "        reward = 0\n",
    "        done = False\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def calculate_terminal_reward(self, action):\n",
    "        obs = np.zeros(571)\n",
    "        reward = self.state.score_by_contract()[self.state.contract_index()]\n",
    "        if self.state.current_player() in {1,3}:\n",
    "            reward = -reward\n",
    "        done = True\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def pick_action(self, action_vector):\n",
    "        action_vector = self.softmax(action_vector)\n",
    "        legal_action_mask = np.array(self.state.legal_actions_mask())[52:52+self.action_space.shape[0]]\n",
    "        masked_action_vector = action_vector*legal_action_mask / sum(action_vector*legal_action_mask)\n",
    "        action = np.random.choice(self.action_space.shape[0], p=masked_action_vector)\n",
    "\n",
    "        if action + 52 not in self.state.legal_actions():\n",
    "            print(action+52, self.state.legal_actions())\n",
    "            print(action_vector[:6])\n",
    "            print(legal_action_mask[:6])\n",
    "            print((action_vector*legal_action_mask)[:6])\n",
    "            print(masked_action_vector[:6])\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        y = np.exp(x - np.max(x))\n",
    "        f_x = y / np.sum(y)\n",
    "        return f_x\n",
    "\n",
    "    def generate_random_game(self): \n",
    "        state = GAME.new_initial_state()\n",
    "        # deal all 52 cards randomly\n",
    "        for i in np.random.choice(52, size=(52,), replace=False):\n",
    "            state.apply_action(i)\n",
    "        return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def get(self):\n",
    "        return self.memory\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "env = BridgeEnv()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.shape[0]\n",
    "n_observations = sum(env.observation_space.shape)\n",
    "\n",
    "actor_critic = BridgeActorCritic('bridge-supervised-epoch=7.ckpt').to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr = LEARNING_RATE)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "trailing_avg_reward = deque()\n",
    "trailing_avg_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0, episode reward: -450, avg_reward: -450.0, episode length: 4\n",
      "episode #1, episode reward: 350, avg_reward: -50.0, episode length: 7\n",
      "episode #2, episode reward: 3400, avg_reward: 1100.0, episode length: 7\n",
      "episode #3, episode reward: 450, avg_reward: 937.5, episode length: 7\n",
      "episode #4, episode reward: 4600, avg_reward: 1670.0, episode length: 8\n",
      "episode #5, episode reward: -350, avg_reward: 1333.33, episode length: 3\n",
      "episode #6, episode reward: -2300, avg_reward: 814.29, episode length: 6\n",
      "episode #7, episode reward: -450, avg_reward: 656.25, episode length: 4\n",
      "episode #8, episode reward: -250, avg_reward: 555.56, episode length: 2\n",
      "episode #9, episode reward: 600, avg_reward: 560.0, episode length: 8\n",
      "episode #10, episode reward: 400, avg_reward: 545.45, episode length: 4\n",
      "episode #11, episode reward: 50, avg_reward: 504.17, episode length: 7\n",
      "episode #12, episode reward: -550, avg_reward: 423.08, episode length: 6\n",
      "episode #13, episode reward: 300, avg_reward: 414.29, episode length: 5\n",
      "episode #14, episode reward: -4000, avg_reward: 120.0, episode length: 4\n",
      "episode #15, episode reward: -4600, avg_reward: -175.0, episode length: 5\n",
      "episode #16, episode reward: 2200, avg_reward: -35.29, episode length: 6\n",
      "episode #17, episode reward: -2800, avg_reward: -188.89, episode length: 5\n",
      "episode #18, episode reward: 2200, avg_reward: -63.16, episode length: 6\n",
      "episode #19, episode reward: -4000, avg_reward: -260.0, episode length: 4\n",
      "episode #20, episode reward: -1600, avg_reward: -323.81, episode length: 7\n",
      "episode #21, episode reward: -350, avg_reward: -325.0, episode length: 3\n",
      "episode #22, episode reward: -1000, avg_reward: -354.35, episode length: 6\n",
      "episode #23, episode reward: -400, avg_reward: -356.25, episode length: 6\n",
      "episode #24, episode reward: -2000, avg_reward: -422.0, episode length: 6\n",
      "episode #25, episode reward: 800, avg_reward: -375.0, episode length: 3\n",
      "episode #26, episode reward: -1600, avg_reward: -420.37, episode length: 4\n",
      "episode #27, episode reward: 350, avg_reward: -392.86, episode length: 6\n",
      "episode #28, episode reward: -2200, avg_reward: -455.17, episode length: 6\n",
      "episode #29, episode reward: -4000, avg_reward: -573.33, episode length: 6\n",
      "episode #30, episode reward: -2200, avg_reward: -625.81, episode length: 6\n",
      "episode #31, episode reward: -2900, avg_reward: -696.88, episode length: 4\n",
      "episode #32, episode reward: 3500, avg_reward: -569.7, episode length: 3\n",
      "episode #33, episode reward: -2200, avg_reward: -617.65, episode length: 5\n",
      "episode #34, episode reward: -500, avg_reward: -614.29, episode length: 5\n",
      "episode #35, episode reward: 3400, avg_reward: -502.78, episode length: 7\n",
      "episode #36, episode reward: -3400, avg_reward: -581.08, episode length: 5\n",
      "episode #37, episode reward: -1600, avg_reward: -607.89, episode length: 6\n",
      "episode #38, episode reward: -2800, avg_reward: -664.1, episode length: 8\n",
      "episode #39, episode reward: 2300, avg_reward: -590.0, episode length: 5\n",
      "episode #40, episode reward: -1100, avg_reward: -602.44, episode length: 5\n",
      "episode #41, episode reward: 2600, avg_reward: -526.19, episode length: 7\n",
      "episode #42, episode reward: -4600, avg_reward: -620.93, episode length: 7\n",
      "episode #43, episode reward: 3400, avg_reward: -529.55, episode length: 10\n",
      "episode #44, episode reward: -1600, avg_reward: -553.33, episode length: 5\n",
      "episode #45, episode reward: 2800, avg_reward: -480.43, episode length: 10\n",
      "episode #46, episode reward: -3400, avg_reward: -542.55, episode length: 5\n",
      "episode #47, episode reward: 3400, avg_reward: -460.42, episode length: 4\n",
      "episode #48, episode reward: 5200, avg_reward: -344.9, episode length: 5\n",
      "episode #49, episode reward: -2200, avg_reward: -382.0, episode length: 5\n",
      "episode #50, episode reward: -600, avg_reward: -386.27, episode length: 3\n",
      "episode #51, episode reward: -1600, avg_reward: -409.62, episode length: 5\n",
      "episode #52, episode reward: -3400, avg_reward: -466.04, episode length: 5\n",
      "episode #53, episode reward: -1600, avg_reward: -487.04, episode length: 6\n",
      "episode #54, episode reward: 250, avg_reward: -473.64, episode length: 4\n",
      "episode #55, episode reward: 4000, avg_reward: -393.75, episode length: 6\n",
      "episode #56, episode reward: 800, avg_reward: -372.81, episode length: 5\n",
      "episode #57, episode reward: -450, avg_reward: -374.14, episode length: 5\n",
      "episode #58, episode reward: -350, avg_reward: -373.73, episode length: 5\n",
      "episode #59, episode reward: 350, avg_reward: -361.67, episode length: 7\n",
      "episode #60, episode reward: 100, avg_reward: -354.1, episode length: 5\n",
      "episode #61, episode reward: 6400, avg_reward: -245.16, episode length: 5\n",
      "episode #62, episode reward: 400, avg_reward: -234.92, episode length: 5\n",
      "episode #63, episode reward: 3400, avg_reward: -178.12, episode length: 7\n",
      "episode #64, episode reward: -400, avg_reward: -181.54, episode length: 5\n",
      "episode #65, episode reward: -4000, avg_reward: -239.39, episode length: 7\n",
      "episode #66, episode reward: -4600, avg_reward: -304.48, episode length: 6\n",
      "episode #67, episode reward: -300, avg_reward: -304.41, episode length: 4\n",
      "episode #68, episode reward: -3400, avg_reward: -349.28, episode length: 3\n",
      "episode #69, episode reward: 2300, avg_reward: -311.43, episode length: 4\n",
      "episode #70, episode reward: -500, avg_reward: -314.08, episode length: 5\n",
      "episode #71, episode reward: -2800, avg_reward: -348.61, episode length: 5\n",
      "episode #72, episode reward: 250, avg_reward: -340.41, episode length: 9\n",
      "episode #73, episode reward: 300, avg_reward: -331.76, episode length: 4\n",
      "episode #74, episode reward: 200, avg_reward: -324.67, episode length: 6\n",
      "episode #75, episode reward: -2200, avg_reward: -349.34, episode length: 4\n",
      "episode #76, episode reward: -2800, avg_reward: -381.17, episode length: 7\n",
      "episode #77, episode reward: 1600, avg_reward: -355.77, episode length: 7\n",
      "episode #78, episode reward: 5200, avg_reward: -285.44, episode length: 6\n",
      "episode #79, episode reward: 2200, avg_reward: -254.38, episode length: 5\n",
      "episode #80, episode reward: -100, avg_reward: -252.47, episode length: 2\n",
      "episode #81, episode reward: -200, avg_reward: -251.83, episode length: 5\n",
      "episode #82, episode reward: 1700, avg_reward: -228.31, episode length: 7\n",
      "episode #83, episode reward: -450, avg_reward: -230.95, episode length: 6\n",
      "episode #84, episode reward: 5800, avg_reward: -160.0, episode length: 6\n",
      "episode #85, episode reward: -1100, avg_reward: -170.93, episode length: 7\n",
      "episode #86, episode reward: -800, avg_reward: -178.16, episode length: 7\n",
      "episode #87, episode reward: 800, avg_reward: -167.05, episode length: 6\n",
      "episode #88, episode reward: -400, avg_reward: -169.66, episode length: 3\n",
      "episode #89, episode reward: -4600, avg_reward: -218.89, episode length: 5\n",
      "episode #90, episode reward: -2200, avg_reward: -240.66, episode length: 4\n",
      "episode #91, episode reward: -1400, avg_reward: -253.26, episode length: 9\n",
      "episode #92, episode reward: 2300, avg_reward: -225.81, episode length: 5\n",
      "episode #93, episode reward: -1100, avg_reward: -235.11, episode length: 5\n",
      "episode #94, episode reward: -1400, avg_reward: -247.37, episode length: 5\n",
      "episode #95, episode reward: -6400, avg_reward: -311.46, episode length: 5\n",
      "episode #96, episode reward: -2200, avg_reward: -330.93, episode length: 7\n",
      "episode #97, episode reward: 1700, avg_reward: -310.2, episode length: 4\n",
      "episode #98, episode reward: 2800, avg_reward: -278.79, episode length: 5\n",
      "episode #99, episode reward: 4600, avg_reward: -230.0, episode length: 4\n",
      "episode #100, episode reward: -2300, avg_reward: -248.5, episode length: 6\n",
      "episode #101, episode reward: -5800, avg_reward: -310.0, episode length: 4\n",
      "episode #102, episode reward: 1100, avg_reward: -333.0, episode length: 3\n",
      "episode #103, episode reward: -200, avg_reward: -339.5, episode length: 5\n",
      "episode #104, episode reward: 300, avg_reward: -382.5, episode length: 4\n",
      "episode #105, episode reward: -4600, avg_reward: -425.0, episode length: 8\n",
      "episode #106, episode reward: -1700, avg_reward: -419.0, episode length: 6\n",
      "episode #107, episode reward: 1400, avg_reward: -400.5, episode length: 5\n",
      "episode #108, episode reward: 1400, avg_reward: -384.0, episode length: 5\n",
      "episode #109, episode reward: 1600, avg_reward: -374.0, episode length: 6\n",
      "episode #110, episode reward: -550, avg_reward: -383.5, episode length: 4\n",
      "episode #111, episode reward: -2800, avg_reward: -412.0, episode length: 3\n",
      "episode #112, episode reward: 2800, avg_reward: -378.5, episode length: 8\n",
      "episode #113, episode reward: -300, avg_reward: -384.5, episode length: 4\n",
      "episode #114, episode reward: -500, avg_reward: -349.5, episode length: 7\n",
      "episode #115, episode reward: 2600, avg_reward: -277.5, episode length: 10\n",
      "episode #116, episode reward: 2200, avg_reward: -277.5, episode length: 8\n",
      "episode #117, episode reward: -1400, avg_reward: -263.5, episode length: 6\n",
      "episode #118, episode reward: 7000, avg_reward: -215.5, episode length: 7\n",
      "episode #119, episode reward: 2600, avg_reward: -149.5, episode length: 4\n",
      "episode #120, episode reward: -3500, avg_reward: -168.5, episode length: 5\n",
      "episode #121, episode reward: -250, avg_reward: -167.5, episode length: 5\n",
      "episode #122, episode reward: 2200, avg_reward: -135.5, episode length: 8\n",
      "episode #123, episode reward: -400, avg_reward: -135.5, episode length: 3\n",
      "episode #124, episode reward: -150, avg_reward: -117.0, episode length: 4\n",
      "episode #125, episode reward: -400, avg_reward: -129.0, episode length: 5\n",
      "episode #126, episode reward: -2800, avg_reward: -141.0, episode length: 8\n",
      "episode #127, episode reward: 1000, avg_reward: -134.5, episode length: 8\n",
      "episode #128, episode reward: 2800, avg_reward: -84.5, episode length: 7\n",
      "episode #129, episode reward: -2600, avg_reward: -70.5, episode length: 7\n",
      "episode #130, episode reward: -1600, avg_reward: -64.5, episode length: 5\n",
      "episode #131, episode reward: 6400, avg_reward: 28.5, episode length: 5\n",
      "episode #132, episode reward: -600, avg_reward: -12.5, episode length: 3\n",
      "episode #133, episode reward: 4600, avg_reward: 55.5, episode length: 7\n",
      "episode #134, episode reward: -1000, avg_reward: 50.5, episode length: 7\n",
      "episode #135, episode reward: 300, avg_reward: 19.5, episode length: 5\n",
      "episode #136, episode reward: -2200, avg_reward: 31.5, episode length: 7\n",
      "episode #137, episode reward: -2000, avg_reward: 27.5, episode length: 3\n",
      "episode #138, episode reward: 5200, avg_reward: 107.5, episode length: 5\n",
      "episode #139, episode reward: 5800, avg_reward: 142.5, episode length: 5\n",
      "episode #140, episode reward: 5200, avg_reward: 205.5, episode length: 5\n",
      "episode #141, episode reward: 1100, avg_reward: 190.5, episode length: 5\n",
      "episode #142, episode reward: -1790, avg_reward: 218.6, episode length: 8\n",
      "episode #143, episode reward: 6400, avg_reward: 248.6, episode length: 6\n",
      "episode #144, episode reward: 200, avg_reward: 266.6, episode length: 5\n",
      "episode #145, episode reward: -5200, avg_reward: 186.6, episode length: 6\n",
      "episode #146, episode reward: -4000, avg_reward: 180.6, episode length: 5\n",
      "episode #147, episode reward: -2200, avg_reward: 124.6, episode length: 5\n",
      "episode #148, episode reward: 2800, avg_reward: 100.6, episode length: 8\n",
      "episode #149, episode reward: -4000, avg_reward: 82.6, episode length: 7\n",
      "episode #150, episode reward: 1000, avg_reward: 98.6, episode length: 7\n",
      "episode #151, episode reward: 3400, avg_reward: 148.6, episode length: 7\n",
      "episode #152, episode reward: 2200, avg_reward: 204.6, episode length: 7\n",
      "episode #153, episode reward: 1700, avg_reward: 237.6, episode length: 3\n",
      "episode #154, episode reward: -4600, avg_reward: 189.1, episode length: 7\n",
      "episode #155, episode reward: -3400, avg_reward: 115.1, episode length: 4\n",
      "episode #156, episode reward: 2200, avg_reward: 129.1, episode length: 4\n",
      "episode #157, episode reward: 1100, avg_reward: 144.6, episode length: 5\n",
      "episode #158, episode reward: -1600, avg_reward: 132.1, episode length: 9\n",
      "episode #159, episode reward: -500, avg_reward: 123.6, episode length: 3\n",
      "episode #160, episode reward: -1700, avg_reward: 105.6, episode length: 4\n",
      "episode #161, episode reward: 4600, avg_reward: 87.6, episode length: 8\n",
      "episode #162, episode reward: 4000, avg_reward: 123.6, episode length: 8\n",
      "episode #163, episode reward: 3400, avg_reward: 123.6, episode length: 7\n",
      "episode #164, episode reward: 1100, avg_reward: 138.6, episode length: 3\n",
      "episode #165, episode reward: 250, avg_reward: 181.1, episode length: 5\n",
      "episode #166, episode reward: -400, avg_reward: 223.1, episode length: 5\n",
      "episode #167, episode reward: 3400, avg_reward: 260.1, episode length: 4\n",
      "episode #168, episode reward: -2800, avg_reward: 266.1, episode length: 4\n",
      "episode #169, episode reward: -5200, avg_reward: 191.1, episode length: 11\n",
      "episode #170, episode reward: 1400, avg_reward: 210.1, episode length: 3\n",
      "episode #171, episode reward: 3500, avg_reward: 273.1, episode length: 3\n",
      "episode #172, episode reward: 1400, avg_reward: 284.6, episode length: 6\n",
      "episode #173, episode reward: 500, avg_reward: 286.6, episode length: 6\n",
      "episode #174, episode reward: -450, avg_reward: 280.1, episode length: 3\n",
      "episode #175, episode reward: -3400, avg_reward: 268.1, episode length: 6\n",
      "episode #176, episode reward: 1600, avg_reward: 312.1, episode length: 8\n",
      "episode #177, episode reward: 3400, avg_reward: 330.1, episode length: 5\n",
      "episode #178, episode reward: -1400, avg_reward: 264.1, episode length: 8\n",
      "episode #179, episode reward: -250, avg_reward: 239.6, episode length: 5\n",
      "episode #180, episode reward: -1100, avg_reward: 229.6, episode length: 3\n",
      "episode #181, episode reward: 5800, avg_reward: 289.6, episode length: 6\n",
      "episode #182, episode reward: -1700, avg_reward: 255.6, episode length: 5\n",
      "episode #183, episode reward: 5800, avg_reward: 318.1, episode length: 8\n",
      "episode #184, episode reward: -5800, avg_reward: 202.1, episode length: 7\n",
      "episode #185, episode reward: 100, avg_reward: 214.1, episode length: 5\n",
      "episode #186, episode reward: -300, avg_reward: 219.1, episode length: 4\n",
      "episode #187, episode reward: 5200, avg_reward: 263.1, episode length: 6\n",
      "episode #188, episode reward: 3200, avg_reward: 299.1, episode length: 4\n",
      "episode #189, episode reward: -6400, avg_reward: 281.1, episode length: 9\n",
      "episode #190, episode reward: -1000, avg_reward: 293.1, episode length: 5\n",
      "episode #191, episode reward: 350, avg_reward: 310.6, episode length: 4\n",
      "episode #192, episode reward: -2800, avg_reward: 259.6, episode length: 5\n",
      "episode #193, episode reward: 5200, avg_reward: 322.6, episode length: 7\n",
      "episode #194, episode reward: -150, avg_reward: 335.1, episode length: 3\n",
      "episode #195, episode reward: -4600, avg_reward: 353.1, episode length: 6\n",
      "episode #196, episode reward: 5800, avg_reward: 433.1, episode length: 8\n",
      "episode #197, episode reward: 4000, avg_reward: 456.1, episode length: 5\n",
      "episode #198, episode reward: -2300, avg_reward: 405.1, episode length: 5\n",
      "episode #199, episode reward: -6400, avg_reward: 295.1, episode length: 11\n",
      "episode #200, episode reward: 1600, avg_reward: 334.1, episode length: 6\n",
      "episode #201, episode reward: -1600, avg_reward: 376.1, episode length: 7\n",
      "episode #202, episode reward: 1600, avg_reward: 381.1, episode length: 6\n",
      "episode #203, episode reward: -3400, avg_reward: 349.1, episode length: 7\n",
      "episode #204, episode reward: -1400, avg_reward: 332.1, episode length: 6\n",
      "episode #205, episode reward: -1600, avg_reward: 362.1, episode length: 7\n",
      "episode #206, episode reward: -600, avg_reward: 373.1, episode length: 4\n",
      "episode #207, episode reward: -5800, avg_reward: 301.1, episode length: 8\n",
      "episode #208, episode reward: 600, avg_reward: 293.1, episode length: 6\n",
      "episode #209, episode reward: -1700, avg_reward: 260.1, episode length: 8\n",
      "episode #210, episode reward: 500, avg_reward: 270.6, episode length: 3\n",
      "episode #211, episode reward: 2000, avg_reward: 318.6, episode length: 3\n",
      "episode #212, episode reward: -2800, avg_reward: 262.6, episode length: 6\n",
      "episode #213, episode reward: 2200, avg_reward: 287.6, episode length: 6\n",
      "episode #214, episode reward: -5200, avg_reward: 240.6, episode length: 5\n",
      "episode #215, episode reward: -1100, avg_reward: 203.6, episode length: 5\n",
      "episode #216, episode reward: 1600, avg_reward: 197.6, episode length: 5\n",
      "episode #217, episode reward: -200, avg_reward: 209.6, episode length: 3\n",
      "episode #218, episode reward: 3400, avg_reward: 173.6, episode length: 6\n",
      "episode #219, episode reward: 5200, avg_reward: 199.6, episode length: 5\n",
      "episode #220, episode reward: -100, avg_reward: 233.6, episode length: 6\n",
      "episode #221, episode reward: -2200, avg_reward: 214.1, episode length: 8\n",
      "episode #222, episode reward: 2300, avg_reward: 215.1, episode length: 5\n",
      "episode #223, episode reward: -300, avg_reward: 216.1, episode length: 5\n",
      "episode #224, episode reward: 1700, avg_reward: 234.6, episode length: 5\n",
      "episode #225, episode reward: -250, avg_reward: 236.1, episode length: 2\n",
      "episode #226, episode reward: 2600, avg_reward: 290.1, episode length: 6\n",
      "episode #227, episode reward: 5800, avg_reward: 338.1, episode length: 7\n",
      "episode #228, episode reward: -3400, avg_reward: 276.1, episode length: 8\n",
      "episode #229, episode reward: 5200, avg_reward: 354.1, episode length: 5\n",
      "episode #230, episode reward: 2200, avg_reward: 392.1, episode length: 6\n",
      "episode #231, episode reward: -3200, avg_reward: 296.1, episode length: 8\n",
      "episode #232, episode reward: -3400, avg_reward: 268.1, episode length: 4\n",
      "episode #233, episode reward: -3400, avg_reward: 188.1, episode length: 8\n",
      "episode #234, episode reward: 1000, avg_reward: 208.1, episode length: 4\n",
      "episode #235, episode reward: 4600, avg_reward: 251.1, episode length: 4\n",
      "episode #236, episode reward: -5800, avg_reward: 215.1, episode length: 5\n",
      "episode #237, episode reward: 5200, avg_reward: 287.1, episode length: 4\n",
      "episode #238, episode reward: 2900, avg_reward: 264.1, episode length: 5\n",
      "episode #239, episode reward: 4000, avg_reward: 246.1, episode length: 7\n",
      "episode #240, episode reward: -400, avg_reward: 190.1, episode length: 3\n",
      "episode #241, episode reward: 5200, avg_reward: 231.1, episode length: 9\n",
      "episode #242, episode reward: 1100, avg_reward: 260.0, episode length: 5\n",
      "episode #243, episode reward: 3400, avg_reward: 230.0, episode length: 7\n",
      "episode #244, episode reward: 5200, avg_reward: 280.0, episode length: 8\n",
      "episode #245, episode reward: 1000, avg_reward: 342.0, episode length: 7\n",
      "episode #246, episode reward: 3400, avg_reward: 416.0, episode length: 6\n",
      "episode #247, episode reward: -450, avg_reward: 433.5, episode length: 4\n",
      "episode #248, episode reward: -2000, avg_reward: 385.5, episode length: 7\n",
      "episode #249, episode reward: -1600, avg_reward: 409.5, episode length: 5\n",
      "episode #250, episode reward: -5800, avg_reward: 341.5, episode length: 6\n",
      "episode #251, episode reward: 100, avg_reward: 308.5, episode length: 3\n",
      "episode #252, episode reward: -2800, avg_reward: 258.5, episode length: 5\n",
      "episode #253, episode reward: -4000, avg_reward: 201.5, episode length: 5\n",
      "episode #254, episode reward: -2800, avg_reward: 219.5, episode length: 6\n",
      "episode #255, episode reward: -4600, avg_reward: 207.5, episode length: 6\n",
      "episode #256, episode reward: 800, avg_reward: 193.5, episode length: 3\n",
      "episode #257, episode reward: -4000, avg_reward: 142.5, episode length: 5\n",
      "episode #258, episode reward: -3400, avg_reward: 124.5, episode length: 8\n",
      "episode #259, episode reward: -600, avg_reward: 123.5, episode length: 4\n",
      "episode #260, episode reward: -450, avg_reward: 136.0, episode length: 6\n",
      "episode #261, episode reward: -3400, avg_reward: 56.0, episode length: 4\n",
      "episode #262, episode reward: 3400, avg_reward: 50.0, episode length: 4\n",
      "episode #263, episode reward: 1000, avg_reward: 26.0, episode length: 6\n",
      "episode #264, episode reward: 250, avg_reward: 17.5, episode length: 6\n",
      "episode #265, episode reward: 2200, avg_reward: 37.0, episode length: 6\n",
      "episode #266, episode reward: -2000, avg_reward: 21.0, episode length: 5\n",
      "episode #267, episode reward: -1400, avg_reward: -27.0, episode length: 6\n",
      "episode #268, episode reward: 5200, avg_reward: 53.0, episode length: 5\n",
      "episode #269, episode reward: -2600, avg_reward: 79.0, episode length: 6\n",
      "episode #270, episode reward: -2800, avg_reward: 37.0, episode length: 5\n",
      "episode #271, episode reward: 3400, avg_reward: 36.0, episode length: 8\n",
      "episode #272, episode reward: 5800, avg_reward: 80.0, episode length: 5\n",
      "episode #273, episode reward: 450, avg_reward: 79.5, episode length: 5\n",
      "episode #274, episode reward: -2200, avg_reward: 62.0, episode length: 4\n",
      "episode #275, episode reward: 1100, avg_reward: 107.0, episode length: 6\n",
      "episode #276, episode reward: 3400, avg_reward: 125.0, episode length: 5\n",
      "episode #277, episode reward: 2600, avg_reward: 117.0, episode length: 6\n",
      "episode #278, episode reward: 300, avg_reward: 134.0, episode length: 5\n",
      "episode #279, episode reward: -4000, avg_reward: 96.5, episode length: 5\n",
      "episode #280, episode reward: 2800, avg_reward: 135.5, episode length: 5\n",
      "episode #281, episode reward: -1400, avg_reward: 63.5, episode length: 4\n",
      "episode #282, episode reward: -4600, avg_reward: 34.5, episode length: 4\n",
      "episode #283, episode reward: -2000, avg_reward: -43.5, episode length: 5\n",
      "episode #284, episode reward: 2200, avg_reward: 36.5, episode length: 6\n",
      "episode #285, episode reward: 1700, avg_reward: 52.5, episode length: 5\n",
      "episode #286, episode reward: 1000, avg_reward: 65.5, episode length: 7\n",
      "episode #287, episode reward: -450, avg_reward: 9.0, episode length: 4\n",
      "episode #288, episode reward: 5200, avg_reward: 29.0, episode length: 5\n",
      "episode #289, episode reward: 4600, avg_reward: 139.0, episode length: 7\n",
      "episode #290, episode reward: 2600, avg_reward: 175.0, episode length: 4\n",
      "episode #291, episode reward: -600, avg_reward: 165.5, episode length: 6\n",
      "episode #292, episode reward: -3400, avg_reward: 159.5, episode length: 5\n",
      "episode #293, episode reward: -3400, avg_reward: 73.5, episode length: 5\n",
      "episode #294, episode reward: 2200, avg_reward: 97.0, episode length: 4\n",
      "episode #295, episode reward: 2300, avg_reward: 166.0, episode length: 6\n",
      "episode #296, episode reward: -500, avg_reward: 103.0, episode length: 5\n",
      "episode #297, episode reward: -500, avg_reward: 58.0, episode length: 6\n",
      "episode #298, episode reward: -350, avg_reward: 77.5, episode length: 8\n",
      "episode #299, episode reward: 5200, avg_reward: 193.5, episode length: 7\n",
      "episode #300, episode reward: 5200, avg_reward: 229.5, episode length: 5\n",
      "episode #301, episode reward: 3400, avg_reward: 279.5, episode length: 6\n",
      "episode #302, episode reward: 5800, avg_reward: 321.5, episode length: 4\n",
      "episode #303, episode reward: 4000, avg_reward: 395.5, episode length: 4\n",
      "episode #304, episode reward: -4600, avg_reward: 363.5, episode length: 6\n",
      "episode #305, episode reward: 1100, avg_reward: 390.5, episode length: 5\n",
      "episode #306, episode reward: -6400, avg_reward: 332.5, episode length: 8\n",
      "episode #307, episode reward: 7000, avg_reward: 460.5, episode length: 8\n",
      "episode #308, episode reward: -2800, avg_reward: 426.5, episode length: 7\n",
      "episode #309, episode reward: -4600, avg_reward: 397.5, episode length: 5\n",
      "episode #310, episode reward: -800, avg_reward: 384.5, episode length: 8\n",
      "episode #311, episode reward: 2300, avg_reward: 387.5, episode length: 6\n",
      "episode #312, episode reward: 1000, avg_reward: 425.5, episode length: 6\n",
      "episode #313, episode reward: 300, avg_reward: 406.5, episode length: 3\n",
      "episode #314, episode reward: -800, avg_reward: 450.5, episode length: 7\n",
      "episode #315, episode reward: 3400, avg_reward: 495.5, episode length: 8\n",
      "episode #316, episode reward: 2200, avg_reward: 501.5, episode length: 7\n",
      "episode #317, episode reward: -4000, avg_reward: 463.5, episode length: 7\n",
      "episode #318, episode reward: -350, avg_reward: 426.0, episode length: 4\n",
      "episode #319, episode reward: -4000, avg_reward: 334.0, episode length: 5\n",
      "episode #320, episode reward: 300, avg_reward: 338.0, episode length: 3\n",
      "episode #321, episode reward: 200, avg_reward: 362.0, episode length: 6\n",
      "episode #322, episode reward: -4000, avg_reward: 299.0, episode length: 7\n",
      "episode #323, episode reward: 1700, avg_reward: 319.0, episode length: 3\n",
      "episode #324, episode reward: 1600, avg_reward: 318.0, episode length: 5\n",
      "episode #325, episode reward: -2600, avg_reward: 294.5, episode length: 7\n",
      "episode #326, episode reward: 250, avg_reward: 271.0, episode length: 4\n",
      "episode #327, episode reward: -3400, avg_reward: 179.0, episode length: 3\n",
      "episode #328, episode reward: 5200, avg_reward: 265.0, episode length: 4\n",
      "episode #329, episode reward: 5800, avg_reward: 271.0, episode length: 7\n",
      "episode #330, episode reward: 2900, avg_reward: 278.0, episode length: 6\n",
      "episode #331, episode reward: -3400, avg_reward: 276.0, episode length: 5\n",
      "episode #332, episode reward: 500, avg_reward: 315.0, episode length: 5\n",
      "episode #333, episode reward: 4000, avg_reward: 389.0, episode length: 6\n",
      "episode #334, episode reward: -250, avg_reward: 376.5, episode length: 3\n",
      "episode #335, episode reward: -500, avg_reward: 325.5, episode length: 6\n",
      "episode #336, episode reward: 2600, avg_reward: 409.5, episode length: 6\n",
      "episode #337, episode reward: 2000, avg_reward: 377.5, episode length: 4\n",
      "episode #338, episode reward: 2200, avg_reward: 370.5, episode length: 8\n",
      "episode #339, episode reward: -2800, avg_reward: 302.5, episode length: 6\n",
      "episode #340, episode reward: 4600, avg_reward: 352.5, episode length: 6\n",
      "episode #341, episode reward: 4600, avg_reward: 346.5, episode length: 7\n",
      "episode #342, episode reward: 5800, avg_reward: 393.5, episode length: 8\n",
      "episode #343, episode reward: 4600, avg_reward: 405.5, episode length: 8\n",
      "episode #344, episode reward: 5200, avg_reward: 405.5, episode length: 8\n",
      "episode #345, episode reward: 200, avg_reward: 397.5, episode length: 4\n",
      "episode #346, episode reward: 5200, avg_reward: 415.5, episode length: 6\n",
      "episode #347, episode reward: 2600, avg_reward: 446.0, episode length: 4\n",
      "episode #348, episode reward: 4600, avg_reward: 512.0, episode length: 10\n",
      "episode #349, episode reward: -5200, avg_reward: 476.0, episode length: 8\n",
      "episode #350, episode reward: -3400, avg_reward: 500.0, episode length: 6\n",
      "episode #351, episode reward: -4600, avg_reward: 453.0, episode length: 8\n",
      "episode #352, episode reward: -2800, avg_reward: 453.0, episode length: 7\n",
      "episode #353, episode reward: 2800, avg_reward: 521.0, episode length: 8\n",
      "episode #354, episode reward: 4000, avg_reward: 589.0, episode length: 6\n",
      "episode #355, episode reward: -2300, avg_reward: 612.0, episode length: 6\n",
      "episode #356, episode reward: -6400, avg_reward: 540.0, episode length: 7\n",
      "episode #357, episode reward: -350, avg_reward: 576.5, episode length: 4\n",
      "episode #358, episode reward: 4000, avg_reward: 650.5, episode length: 4\n",
      "episode #359, episode reward: -2200, avg_reward: 634.5, episode length: 6\n",
      "episode #360, episode reward: -2300, avg_reward: 616.0, episode length: 10\n",
      "episode #361, episode reward: 3400, avg_reward: 684.0, episode length: 5\n",
      "episode #362, episode reward: -600, avg_reward: 644.0, episode length: 5\n",
      "episode #363, episode reward: 6400, avg_reward: 698.0, episode length: 6\n",
      "episode #364, episode reward: 350, avg_reward: 699.0, episode length: 6\n",
      "episode #365, episode reward: -4000, avg_reward: 637.0, episode length: 7\n",
      "episode #366, episode reward: 500, avg_reward: 662.0, episode length: 6\n",
      "episode #367, episode reward: -1700, avg_reward: 659.0, episode length: 3\n",
      "episode #368, episode reward: -450, avg_reward: 602.5, episode length: 6\n",
      "episode #369, episode reward: 5200, avg_reward: 680.5, episode length: 9\n",
      "episode #370, episode reward: -4600, avg_reward: 662.5, episode length: 8\n",
      "episode #371, episode reward: 1600, avg_reward: 644.5, episode length: 6\n",
      "episode #372, episode reward: -6400, avg_reward: 522.5, episode length: 5\n",
      "episode #373, episode reward: -4000, avg_reward: 478.0, episode length: 4\n",
      "episode #374, episode reward: -300, avg_reward: 497.0, episode length: 7\n",
      "episode #375, episode reward: -550, avg_reward: 480.5, episode length: 4\n",
      "episode #376, episode reward: 4000, avg_reward: 486.5, episode length: 7\n",
      "episode #377, episode reward: 450, avg_reward: 465.0, episode length: 4\n",
      "episode #378, episode reward: 4000, avg_reward: 502.0, episode length: 6\n",
      "episode #379, episode reward: 6400, avg_reward: 606.0, episode length: 6\n",
      "episode #380, episode reward: 5200, avg_reward: 630.0, episode length: 5\n",
      "episode #381, episode reward: 4000, avg_reward: 684.0, episode length: 6\n",
      "episode #382, episode reward: 3500, avg_reward: 765.0, episode length: 6\n",
      "episode #383, episode reward: -300, avg_reward: 782.0, episode length: 4\n",
      "episode #384, episode reward: -2200, avg_reward: 738.0, episode length: 6\n",
      "episode #385, episode reward: -250, avg_reward: 718.5, episode length: 3\n",
      "episode #386, episode reward: 600, avg_reward: 714.5, episode length: 4\n",
      "episode #387, episode reward: -300, avg_reward: 716.0, episode length: 4\n",
      "episode #388, episode reward: 200, avg_reward: 666.0, episode length: 8\n",
      "episode #389, episode reward: -300, avg_reward: 617.0, episode length: 4\n",
      "episode #390, episode reward: 5200, avg_reward: 643.0, episode length: 7\n",
      "episode #391, episode reward: 2300, avg_reward: 672.0, episode length: 4\n",
      "episode #392, episode reward: -2200, avg_reward: 684.0, episode length: 5\n",
      "episode #393, episode reward: 500, avg_reward: 723.0, episode length: 6\n",
      "episode #394, episode reward: 1000, avg_reward: 711.0, episode length: 8\n",
      "episode #395, episode reward: 450, avg_reward: 692.5, episode length: 6\n",
      "episode #396, episode reward: -4600, avg_reward: 651.5, episode length: 6\n",
      "episode #397, episode reward: 2200, avg_reward: 678.5, episode length: 7\n",
      "episode #398, episode reward: 1100, avg_reward: 693.0, episode length: 6\n",
      "episode #399, episode reward: 1600, avg_reward: 657.0, episode length: 8\n",
      "episode #400, episode reward: -4600, avg_reward: 559.0, episode length: 5\n",
      "episode #401, episode reward: 1700, avg_reward: 542.0, episode length: 7\n",
      "episode #402, episode reward: -200, avg_reward: 482.0, episode length: 7\n",
      "episode #403, episode reward: 2800, avg_reward: 470.0, episode length: 4\n",
      "episode #404, episode reward: 300, avg_reward: 519.0, episode length: 3\n",
      "episode #405, episode reward: -2000, avg_reward: 488.0, episode length: 4\n",
      "episode #406, episode reward: -450, avg_reward: 547.5, episode length: 3\n",
      "episode #407, episode reward: -4000, avg_reward: 437.5, episode length: 5\n",
      "episode #408, episode reward: 5800, avg_reward: 523.5, episode length: 8\n",
      "episode #409, episode reward: 450, avg_reward: 574.0, episode length: 4\n",
      "episode #410, episode reward: -4000, avg_reward: 542.0, episode length: 6\n",
      "episode #411, episode reward: 5200, avg_reward: 571.0, episode length: 7\n",
      "episode #412, episode reward: 1700, avg_reward: 578.0, episode length: 3\n",
      "episode #413, episode reward: -2200, avg_reward: 553.0, episode length: 6\n",
      "episode #414, episode reward: 5200, avg_reward: 613.0, episode length: 8\n",
      "episode #415, episode reward: -300, avg_reward: 576.0, episode length: 4\n",
      "episode #416, episode reward: 1000, avg_reward: 564.0, episode length: 9\n",
      "episode #417, episode reward: 200, avg_reward: 606.0, episode length: 7\n",
      "episode #418, episode reward: -2000, avg_reward: 589.5, episode length: 10\n",
      "episode #419, episode reward: 1100, avg_reward: 640.5, episode length: 4\n",
      "episode #420, episode reward: -5200, avg_reward: 585.5, episode length: 5\n",
      "episode #421, episode reward: 2600, avg_reward: 609.5, episode length: 4\n",
      "episode #422, episode reward: -150, avg_reward: 648.0, episode length: 3\n",
      "episode #423, episode reward: -3400, avg_reward: 597.0, episode length: 7\n",
      "episode #424, episode reward: 1700, avg_reward: 598.0, episode length: 4\n",
      "episode #425, episode reward: 2800, avg_reward: 652.0, episode length: 8\n",
      "episode #426, episode reward: 3400, avg_reward: 683.5, episode length: 7\n",
      "episode #427, episode reward: 3400, avg_reward: 751.5, episode length: 5\n",
      "episode #428, episode reward: -2600, avg_reward: 673.5, episode length: 5\n",
      "episode #429, episode reward: -200, avg_reward: 613.5, episode length: 6\n",
      "episode #430, episode reward: -4600, avg_reward: 538.5, episode length: 7\n",
      "episode #431, episode reward: -450, avg_reward: 568.0, episode length: 2\n",
      "episode #432, episode reward: -2300, avg_reward: 540.0, episode length: 5\n",
      "episode #433, episode reward: 2300, avg_reward: 523.0, episode length: 6\n",
      "episode #434, episode reward: 1600, avg_reward: 541.5, episode length: 7\n",
      "episode #435, episode reward: 2000, avg_reward: 566.5, episode length: 7\n",
      "episode #436, episode reward: 2900, avg_reward: 569.5, episode length: 4\n",
      "episode #437, episode reward: 500, avg_reward: 554.5, episode length: 5\n",
      "episode #438, episode reward: 2800, avg_reward: 560.5, episode length: 6\n",
      "episode #439, episode reward: -300, avg_reward: 585.5, episode length: 4\n",
      "episode #440, episode reward: -800, avg_reward: 531.5, episode length: 5\n",
      "episode #441, episode reward: 4000, avg_reward: 525.5, episode length: 5\n",
      "episode #442, episode reward: -4600, avg_reward: 421.5, episode length: 5\n",
      "episode #443, episode reward: 800, avg_reward: 383.5, episode length: 4\n",
      "episode #444, episode reward: -1400, avg_reward: 317.5, episode length: 7\n",
      "episode #445, episode reward: 2200, avg_reward: 337.5, episode length: 5\n",
      "episode #446, episode reward: 4600, avg_reward: 331.5, episode length: 8\n",
      "episode #447, episode reward: 1600, avg_reward: 321.5, episode length: 5\n",
      "episode #448, episode reward: 4000, avg_reward: 315.5, episode length: 7\n",
      "episode #449, episode reward: -300, avg_reward: 364.5, episode length: 5\n",
      "episode #450, episode reward: 2800, avg_reward: 426.5, episode length: 6\n",
      "episode #451, episode reward: 4600, avg_reward: 518.5, episode length: 8\n",
      "episode #452, episode reward: -1700, avg_reward: 529.5, episode length: 7\n",
      "episode #453, episode reward: -2900, avg_reward: 472.5, episode length: 6\n",
      "episode #454, episode reward: -5800, avg_reward: 374.5, episode length: 5\n",
      "episode #455, episode reward: 4000, avg_reward: 437.5, episode length: 7\n",
      "episode #456, episode reward: -1400, avg_reward: 487.5, episode length: 5\n",
      "episode #457, episode reward: -5800, avg_reward: 433.0, episode length: 6\n",
      "episode #458, episode reward: -1700, avg_reward: 376.0, episode length: 5\n",
      "episode #459, episode reward: -2900, avg_reward: 369.0, episode length: 7\n",
      "episode #460, episode reward: 5800, avg_reward: 450.0, episode length: 6\n",
      "episode #461, episode reward: -2900, avg_reward: 387.0, episode length: 5\n",
      "episode #462, episode reward: 1000, avg_reward: 403.0, episode length: 8\n",
      "episode #463, episode reward: -2200, avg_reward: 317.0, episode length: 7\n",
      "episode #464, episode reward: -5800, avg_reward: 255.5, episode length: 5\n",
      "episode #465, episode reward: -1700, avg_reward: 278.5, episode length: 4\n",
      "episode #466, episode reward: 150, avg_reward: 275.0, episode length: 6\n",
      "episode #467, episode reward: -1700, avg_reward: 275.0, episode length: 6\n",
      "episode #468, episode reward: -250, avg_reward: 277.0, episode length: 2\n",
      "episode #469, episode reward: 4600, avg_reward: 271.0, episode length: 7\n",
      "episode #470, episode reward: 4000, avg_reward: 357.0, episode length: 6\n",
      "episode #471, episode reward: 100, avg_reward: 342.0, episode length: 6\n",
      "episode #472, episode reward: -2200, avg_reward: 384.0, episode length: 5\n",
      "episode #473, episode reward: 300, avg_reward: 427.0, episode length: 5\n",
      "episode #474, episode reward: 2600, avg_reward: 456.0, episode length: 7\n",
      "episode #475, episode reward: -2800, avg_reward: 433.5, episode length: 5\n",
      "episode #476, episode reward: -2200, avg_reward: 371.5, episode length: 7\n",
      "episode #477, episode reward: 2800, avg_reward: 395.0, episode length: 7\n",
      "episode #478, episode reward: 1600, avg_reward: 371.0, episode length: 5\n",
      "episode #479, episode reward: 2300, avg_reward: 330.0, episode length: 9\n",
      "episode #480, episode reward: 4600, avg_reward: 324.0, episode length: 8\n",
      "episode #481, episode reward: 2200, avg_reward: 306.0, episode length: 5\n",
      "episode #482, episode reward: 4600, avg_reward: 317.0, episode length: 5\n",
      "episode #483, episode reward: 1400, avg_reward: 334.0, episode length: 5\n",
      "episode #484, episode reward: 5200, avg_reward: 408.0, episode length: 6\n",
      "episode #485, episode reward: 2600, avg_reward: 436.5, episode length: 12\n",
      "episode #486, episode reward: 2000, avg_reward: 450.5, episode length: 7\n",
      "episode #487, episode reward: -5200, avg_reward: 401.5, episode length: 6\n",
      "episode #488, episode reward: 4600, avg_reward: 445.5, episode length: 5\n",
      "episode #489, episode reward: -1100, avg_reward: 437.5, episode length: 7\n",
      "episode #490, episode reward: -800, avg_reward: 377.5, episode length: 8\n",
      "episode #491, episode reward: 2800, avg_reward: 382.5, episode length: 7\n",
      "episode #492, episode reward: -5800, avg_reward: 346.5, episode length: 9\n",
      "episode #493, episode reward: -350, avg_reward: 338.0, episode length: 5\n",
      "episode #494, episode reward: 3400, avg_reward: 362.0, episode length: 6\n",
      "episode #495, episode reward: 200, avg_reward: 359.5, episode length: 5\n",
      "episode #496, episode reward: -400, avg_reward: 401.5, episode length: 6\n",
      "episode #497, episode reward: 2800, avg_reward: 407.5, episode length: 6\n",
      "episode #498, episode reward: 6400, avg_reward: 460.5, episode length: 7\n",
      "episode #499, episode reward: 2200, avg_reward: 466.5, episode length: 8\n",
      "episode #500, episode reward: 2000, avg_reward: 532.5, episode length: 3\n",
      "episode #501, episode reward: -2800, avg_reward: 487.5, episode length: 6\n",
      "episode #502, episode reward: -300, avg_reward: 486.5, episode length: 2\n",
      "episode #503, episode reward: -5800, avg_reward: 400.5, episode length: 9\n",
      "episode #504, episode reward: 5200, avg_reward: 449.5, episode length: 7\n",
      "episode #505, episode reward: -600, avg_reward: 463.5, episode length: 5\n",
      "episode #506, episode reward: 2200, avg_reward: 490.0, episode length: 6\n",
      "episode #507, episode reward: -550, avg_reward: 524.5, episode length: 6\n",
      "episode #508, episode reward: 2800, avg_reward: 494.5, episode length: 7\n",
      "episode #509, episode reward: 400, avg_reward: 494.0, episode length: 6\n",
      "episode #510, episode reward: 2800, avg_reward: 562.0, episode length: 9\n",
      "episode #511, episode reward: 450, avg_reward: 514.5, episode length: 5\n",
      "episode #512, episode reward: 250, avg_reward: 500.0, episode length: 5\n",
      "episode #513, episode reward: 1100, avg_reward: 533.0, episode length: 3\n",
      "episode #514, episode reward: -800, avg_reward: 473.0, episode length: 5\n",
      "episode #515, episode reward: -250, avg_reward: 473.5, episode length: 3\n",
      "episode #516, episode reward: -1700, avg_reward: 446.5, episode length: 7\n",
      "episode #517, episode reward: 200, avg_reward: 446.5, episode length: 7\n",
      "episode #518, episode reward: -1700, avg_reward: 449.5, episode length: 6\n",
      "episode #519, episode reward: 100, avg_reward: 439.5, episode length: 5\n",
      "episode #520, episode reward: 1400, avg_reward: 505.5, episode length: 4\n",
      "episode #521, episode reward: -500, avg_reward: 474.5, episode length: 3\n",
      "episode #522, episode reward: 500, avg_reward: 481.0, episode length: 3\n",
      "episode #523, episode reward: -800, avg_reward: 507.0, episode length: 8\n",
      "episode #524, episode reward: 350, avg_reward: 493.5, episode length: 7\n",
      "episode #525, episode reward: -2200, avg_reward: 443.5, episode length: 6\n",
      "episode #526, episode reward: 450, avg_reward: 414.0, episode length: 4\n",
      "episode #527, episode reward: -800, avg_reward: 372.0, episode length: 5\n",
      "episode #528, episode reward: -2200, avg_reward: 376.0, episode length: 5\n",
      "episode #529, episode reward: -200, avg_reward: 376.0, episode length: 3\n",
      "episode #530, episode reward: 2200, avg_reward: 444.0, episode length: 8\n",
      "episode #531, episode reward: 2300, avg_reward: 471.5, episode length: 6\n",
      "episode #532, episode reward: -3400, avg_reward: 460.5, episode length: 5\n",
      "episode #533, episode reward: 450, avg_reward: 442.0, episode length: 5\n",
      "episode #534, episode reward: 450, avg_reward: 430.5, episode length: 3\n",
      "episode #535, episode reward: -550, avg_reward: 405.0, episode length: 4\n",
      "episode #536, episode reward: 3400, avg_reward: 410.0, episode length: 5\n",
      "episode #537, episode reward: -500, avg_reward: 400.0, episode length: 5\n",
      "episode #538, episode reward: -350, avg_reward: 368.5, episode length: 3\n",
      "episode #539, episode reward: 450, avg_reward: 376.0, episode length: 4\n",
      "episode #540, episode reward: 1100, avg_reward: 395.0, episode length: 4\n",
      "episode #541, episode reward: -1400, avg_reward: 341.0, episode length: 5\n",
      "episode #542, episode reward: -2200, avg_reward: 365.0, episode length: 6\n",
      "episode #543, episode reward: 2000, avg_reward: 377.0, episode length: 7\n",
      "episode #544, episode reward: 300, avg_reward: 394.0, episode length: 3\n",
      "episode #545, episode reward: 400, avg_reward: 376.0, episode length: 3\n",
      "episode #546, episode reward: 300, avg_reward: 333.0, episode length: 4\n",
      "episode #547, episode reward: 450, avg_reward: 321.5, episode length: 4\n",
      "episode #548, episode reward: 1000, avg_reward: 291.5, episode length: 8\n",
      "episode #549, episode reward: 250, avg_reward: 297.0, episode length: 5\n",
      "episode #550, episode reward: -3400, avg_reward: 235.0, episode length: 7\n",
      "episode #551, episode reward: -2200, avg_reward: 167.0, episode length: 5\n",
      "episode #552, episode reward: 4000, avg_reward: 224.0, episode length: 6\n",
      "episode #553, episode reward: -300, avg_reward: 250.0, episode length: 6\n",
      "episode #554, episode reward: -6400, avg_reward: 244.0, episode length: 6\n",
      "episode #555, episode reward: 550, avg_reward: 209.5, episode length: 4\n",
      "episode #556, episode reward: 450, avg_reward: 228.0, episode length: 4\n",
      "episode #557, episode reward: 4000, avg_reward: 326.0, episode length: 5\n",
      "episode #558, episode reward: 500, avg_reward: 348.0, episode length: 4\n",
      "episode #559, episode reward: -250, avg_reward: 374.5, episode length: 6\n",
      "episode #560, episode reward: 1600, avg_reward: 332.5, episode length: 6\n",
      "episode #561, episode reward: 500, avg_reward: 366.5, episode length: 3\n",
      "episode #562, episode reward: 1100, avg_reward: 367.5, episode length: 5\n",
      "episode #563, episode reward: 350, avg_reward: 393.0, episode length: 4\n",
      "episode #564, episode reward: -2000, avg_reward: 431.0, episode length: 5\n",
      "episode #565, episode reward: 200, avg_reward: 450.0, episode length: 4\n",
      "episode #566, episode reward: 50, avg_reward: 449.0, episode length: 4\n",
      "episode #567, episode reward: 150, avg_reward: 467.5, episode length: 5\n",
      "episode #568, episode reward: 300, avg_reward: 473.0, episode length: 4\n",
      "episode #569, episode reward: 1600, avg_reward: 443.0, episode length: 8\n",
      "episode #570, episode reward: -500, avg_reward: 398.0, episode length: 5\n",
      "episode #571, episode reward: 2800, avg_reward: 425.0, episode length: 6\n",
      "episode #572, episode reward: 4000, avg_reward: 487.0, episode length: 5\n",
      "episode #573, episode reward: 650, avg_reward: 490.5, episode length: 4\n",
      "episode #574, episode reward: -1100, avg_reward: 453.5, episode length: 5\n",
      "episode #575, episode reward: -1400, avg_reward: 467.5, episode length: 5\n",
      "episode #576, episode reward: -2200, avg_reward: 467.5, episode length: 7\n",
      "episode #577, episode reward: -1400, avg_reward: 425.5, episode length: 6\n",
      "episode #578, episode reward: 250, avg_reward: 412.0, episode length: 5\n",
      "episode #579, episode reward: -5200, avg_reward: 337.0, episode length: 6\n",
      "episode #580, episode reward: -5200, avg_reward: 239.0, episode length: 6\n",
      "episode #581, episode reward: -1520, avg_reward: 201.8, episode length: 4\n",
      "episode #582, episode reward: 4000, avg_reward: 195.8, episode length: 5\n",
      "episode #583, episode reward: -2800, avg_reward: 153.8, episode length: 5\n",
      "episode #584, episode reward: -200, avg_reward: 99.8, episode length: 2\n",
      "episode #585, episode reward: -3400, avg_reward: 39.8, episode length: 6\n",
      "episode #586, episode reward: -350, avg_reward: 16.3, episode length: 4\n",
      "episode #587, episode reward: -800, avg_reward: 60.3, episode length: 7\n",
      "episode #588, episode reward: 350, avg_reward: 17.8, episode length: 5\n",
      "episode #589, episode reward: 4000, avg_reward: 68.8, episode length: 8\n",
      "episode #590, episode reward: 200, avg_reward: 78.8, episode length: 6\n",
      "episode #591, episode reward: 200, avg_reward: 52.8, episode length: 5\n",
      "episode #592, episode reward: 400, avg_reward: 114.8, episode length: 3\n",
      "episode #593, episode reward: 400, avg_reward: 122.3, episode length: 6\n",
      "episode #594, episode reward: -150, avg_reward: 86.8, episode length: 4\n",
      "episode #595, episode reward: 450, avg_reward: 89.3, episode length: 5\n",
      "episode #596, episode reward: -1400, avg_reward: 79.3, episode length: 8\n",
      "episode #597, episode reward: 200, avg_reward: 53.3, episode length: 7\n",
      "episode #598, episode reward: 450, avg_reward: -6.2, episode length: 7\n",
      "episode #599, episode reward: 450, avg_reward: -23.7, episode length: 4\n",
      "episode #600, episode reward: 500, avg_reward: -38.7, episode length: 3\n",
      "episode #601, episode reward: 4000, avg_reward: 29.3, episode length: 8\n",
      "episode #602, episode reward: 400, avg_reward: 36.3, episode length: 7\n",
      "episode #603, episode reward: -1700, avg_reward: 77.3, episode length: 6\n",
      "episode #604, episode reward: 1600, avg_reward: 41.3, episode length: 4\n",
      "episode #605, episode reward: 400, avg_reward: 51.3, episode length: 5\n",
      "episode #606, episode reward: 450, avg_reward: 33.8, episode length: 3\n",
      "episode #607, episode reward: 200, avg_reward: 41.3, episode length: 5\n",
      "episode #608, episode reward: 1100, avg_reward: 24.3, episode length: 7\n",
      "episode #609, episode reward: -4000, avg_reward: -19.7, episode length: 7\n",
      "episode #610, episode reward: 3400, avg_reward: -13.7, episode length: 7\n",
      "episode #611, episode reward: 300, avg_reward: -15.2, episode length: 4\n",
      "episode #612, episode reward: 2600, avg_reward: 8.3, episode length: 5\n",
      "episode #613, episode reward: -250, avg_reward: -5.2, episode length: 6\n",
      "episode #614, episode reward: -250, avg_reward: 0.3, episode length: 5\n",
      "episode #615, episode reward: 350, avg_reward: 6.3, episode length: 4\n",
      "episode #616, episode reward: -1100, avg_reward: 12.3, episode length: 4\n",
      "episode #617, episode reward: 50, avg_reward: 10.8, episode length: 3\n",
      "episode #618, episode reward: 450, avg_reward: 32.3, episode length: 6\n",
      "episode #619, episode reward: 3400, avg_reward: 65.3, episode length: 5\n",
      "episode #620, episode reward: -2200, avg_reward: 29.3, episode length: 4\n",
      "episode #621, episode reward: -300, avg_reward: 31.3, episode length: 5\n",
      "episode #622, episode reward: -1700, avg_reward: 9.3, episode length: 4\n",
      "episode #623, episode reward: -450, avg_reward: 12.8, episode length: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8316/1975414670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Initialize the environment and state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8316/809102096.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_random_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8316/809102096.py\u001b[0m in \u001b[0;36mgenerate_random_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# deal all 52 cards randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).to(device).float().unsqueeze(0)\n",
    "    for t in count():\n",
    "        value, policy_dist = actor_critic.forward(state)\n",
    "\n",
    "        value = value.detach().numpy()[0,0]\n",
    "        dist = policy_dist.detach().squeeze().numpy()\n",
    "\n",
    "        new_state, reward, done, metadata = env.step(dist)\n",
    "        new_state = torch.from_numpy(new_state).to(device).float().unsqueeze(0)\n",
    "\n",
    "        action = metadata[\"action\"]\n",
    "        log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "\n",
    "            Qval, _ = actor_critic.forward(new_state)\n",
    "\n",
    "            trailing_avg_reward.append(reward)\n",
    "            if len(trailing_avg_reward) > trailing_avg_size:\n",
    "                trailing_avg_reward.popleft()\n",
    "\n",
    "            \n",
    "            print(f\"episode #{i_episode}, episode reward: {reward}, avg_reward: {round(np.mean(trailing_avg_reward),2)}, episode length: {t+1}\")\n",
    "\n",
    "\n",
    "            #print(env.state)\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t]/100 + GAMMA * Qval\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    values = torch.FloatTensor(values) # values calculated by Critic\n",
    "    Qvals = torch.FloatTensor(Qvals) # real values (calculated by sum of episode reward * discount factor)\n",
    "    log_probs = torch.stack(log_probs) # log probability of each move in the episode\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss =  (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print('Complete')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "752579dbebe7f4dfe7c1aa72eac13e23fc88be2cc1ea7ab14e1f8d69b2d97d12"
  },
  "kernelspec": {
   "display_name": "Gruff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
