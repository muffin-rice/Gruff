{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import gym, gym.spaces\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "%reload_ext cython\n",
    "# import line_profiler\n",
    "# %load_ext line_profiler\n",
    "\n",
    "from BridgeNetwork import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%cython -f --compile-args=-DCYTHON_TRACE=1\n",
    "# cython: linetrace=True\n",
    "\n",
    "\n",
    "%%cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import gym, gym.spaces\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from BridgeNetwork import *\n",
    "GAME = pyspiel.load_game('bridge(use_double_dummy_result=true)')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Adversary(ABC):\n",
    "    def getAction(observation):\n",
    "        raise NotImplemented\n",
    "\n",
    "class RandomAdversary(Adversary):\n",
    "    def __init__(self, possible_actions):\n",
    "        self.possible_actions = possible_actions\n",
    "    def get_action(self, observation):\n",
    "        return np.random.rand(self.possible_actions)\n",
    "\n",
    "class PolicyNetAdversary(Adversary):\n",
    "    def __init__(self, policy_net):\n",
    "        self.policy_net = policy_net\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        with torch.no_grad():\n",
    "            self.policy_net.eval()\n",
    "            observation = torch.from_numpy(observation).to(device).float().unsqueeze(0)\n",
    "            return self.policy_net(observation).cpu().numpy()\n",
    "\n",
    "\n",
    "class WeightedRandomSelectedAdversary(Adversary):\n",
    "    def __init__(self, adversaries, weights = None) -> None:\n",
    "        self.adversaries = adversaries\n",
    "        self.weights = weights\n",
    "\n",
    "        if self.weights == None:\n",
    "            self.weights = np.full(len(self.adversaries), 1/len(self.adversaries))  \n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        return random.choices(self.adversaries, weights=self.weights, k=1)[0].get_action(observation)\n",
    "        \n",
    "\n",
    "class AdversarialBridgeEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "\n",
    "    def __init__(self, adversary, adversary_plays_first = False):\n",
    "        super(AdversarialBridgeEnv, self).__init__()    # Define action and observation space\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(38,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(571,), dtype=np.float32)\n",
    "        self.adversary = adversary\n",
    "        self.adversary_plays_first = adversary_plays_first\n",
    "        self.reset(adversary_plays_first = adversary_plays_first)\n",
    "\n",
    "    def reset(self, adversary = None, adversary_plays_first = False):\n",
    "        self.adversary = adversary if adversary != None else self.adversary\n",
    "        self.adversary_plays_first = adversary_plays_first\n",
    "        self.state = self.generate_random_game()\n",
    "        if self.adversary_plays_first:\n",
    "            self.adversary_step()\n",
    "\n",
    "        return np.array(self.state.observation_tensor())\n",
    "\n",
    "    def step(self, action_dist):\n",
    "        action = self.pick_action(action_dist)\n",
    "        self.state.apply_action(action+52)\n",
    "        \n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action)\n",
    "        \n",
    "        # opposing team action\n",
    "        self.adversary_step()\n",
    "\n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action)\n",
    "\n",
    "        return self.calculate_default_reward(action)\n",
    "\n",
    "    def adversary_step(self):\n",
    "        self.state.apply_action(\n",
    "            self.pick_action(\n",
    "                self.adversary.get_action(np.array(self.state.observation_tensor()))\n",
    "            ) + 52\n",
    "        )\n",
    "\n",
    "    def calculate_default_reward(self, action):\n",
    "        obs = np.array(self.state.observation_tensor())\n",
    "        reward = 0\n",
    "        done = False\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def calculate_terminal_reward(self, action):\n",
    "        obs = np.zeros(571)\n",
    "        all_possible_contract_rewards = self.state.score_by_contract()\n",
    "        reward = all_possible_contract_rewards[self.state.contract_index()]\n",
    "        max_possible_reward = max(all_possible_contract_rewards)\n",
    "        min_possible_reward = min(all_possible_contract_rewards)\n",
    "        reward = (reward-min_possible_reward)/(max_possible_reward-min_possible_reward)\n",
    "        # bidding ends on the winning-bidder's turn, since the bid is won after 3 passes and it becomes the 4th player's turn again\n",
    "        # if the adversary goes first, the adversary won the bid if the current player is 0 or 2, else 1 or 3\n",
    "        if self.state.current_player() in ({0,2} if self.adversary_plays_first else {1,3}):\n",
    "            # Reward was calculated from adversary's perspective\n",
    "            reward = 1-reward\n",
    "        done = True\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def pick_action(self, action_vector):\n",
    "        action_vector = self.softmax(action_vector)\n",
    "        legal_action_mask = np.array(self.state.legal_actions_mask())[52:52+self.action_space.shape[0]]\n",
    "        masked_action_vector = action_vector*legal_action_mask\n",
    "        action = np.argmax(masked_action_vector)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        y = np.exp(x - np.max(x))\n",
    "        f_x = y / np.sum(y)\n",
    "        return f_x\n",
    "\n",
    "    def generate_random_game(self): \n",
    "        state = GAME.new_initial_state()\n",
    "        # deal all 52 cards randomly\n",
    "        for i in np.random.choice(52, size=(52,), replace=False):\n",
    "            state.apply_action(i)\n",
    "        return state\n",
    "\n",
    "class MultipleSimulationRewardEnv(AdversarialBridgeEnv):\n",
    "\n",
    "    def reset(self, adversary = None, adversary_plays_first = False):\n",
    "        self.adversary = adversary if adversary != None else self.adversary\n",
    "        self.adversary_plays_first = adversary_plays_first\n",
    "        self.games = self.generate_random_games()\n",
    "        self.state = self.games[0]\n",
    "        if self.adversary_plays_first:\n",
    "            self.adversary_step()\n",
    "\n",
    "        return np.array(self.state.observation_tensor())\n",
    "\n",
    "    def calculate_terminal_reward(self, action):\n",
    "        obs = np.zeros(571)\n",
    "        gen = list((np.array(state.score_by_contract()) for state in self.games))\n",
    "        all_state_contracts = np.stack(gen)\n",
    "        all_possible_contract_rewards = np.mean(all_state_contracts, axis=0)\n",
    "        reward = all_possible_contract_rewards[self.state.contract_index()]\n",
    "        # max_possible_reward = max(all_possible_contract_rewards)\n",
    "        # min_possible_reward = min(all_possible_contract_rewards)\n",
    "        # reward = (reward-min_possible_reward)/(max_possible_reward-min_possible_reward)\n",
    "        # bidding ends on the winning-bidder's turn, since the bid is won after 3 passes and it becomes the 4th player's turn again\n",
    "        # if the adversary goes first, the adversary won the bid if the current player is 0 or 2, else 1 or 3\n",
    "        if self.state.current_player() in ({0,2} if self.adversary_plays_first else {1,3}):\n",
    "            # Reward was calculated from adversary's perspective\n",
    "            reward = -reward\n",
    "        done = True\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "        \n",
    "    def generate_random_games(self, n_games=5): \n",
    "        games = [GAME.new_initial_state() for i in range(n_games)]\n",
    "\n",
    "        random_deal = np.random.choice(52, size=(52,), replace=False)\n",
    "\n",
    "        player_hand = random_deal[:26]\n",
    "        opponent_cards =  random_deal[26:]\n",
    "        opponent_hands = [np.random.permutation(opponent_cards) for i in range(n_games)]\n",
    "\n",
    "        # deal all 52 cards randomly\n",
    "        deal_order = np.empty(52, dtype=int)\n",
    "        for game, opponent_hand in zip(games,opponent_hands):\n",
    "            if (self.adversary_plays_first): \n",
    "                deal_order[0::2] = opponent_hand\n",
    "                deal_order[1::2] = player_hand\n",
    "            else:\n",
    "                deal_order[0::2] = player_hand\n",
    "                deal_order[1::2] = opponent_hand\n",
    "            for i in range(52):\n",
    "                game.apply_action(deal_order[i])\n",
    "        return games\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, env):\n",
    "    global steps_done\n",
    "    global ucb_action_picked_counter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ucb_term = UCB_CONFIDENCE*(np.sqrt(np.log(steps_done)/ucb_action_picked_counter))\n",
    "        steps_done += 1\n",
    "        policy_net.eval()\n",
    "        return policy_net(state).cpu()+ucb_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "                                                \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch.type(torch.int64).to(device).unsqueeze(1)).squeeze()\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    target_net.train()\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "GAMMA = 1\n",
    "\n",
    "TARGET_UPDATE = 100\n",
    "\n",
    "ADVERSARY_RANDOMNESS_DECAY = 0.90\n",
    "ADVERSARY_RANDOMNESS = 1.0\n",
    "ADVERSARY_UPDATE = 100\n",
    "\n",
    "UCB_CONFIDENCE = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = []\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "\n",
    "policy_net = BridgeSupervised().to(device)\n",
    "target_net = BridgeSupervised().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 1\n",
    "ucb_action_picked_counter=np.ones((38))\n",
    "\n",
    "trailing_avg_reward = deque()\n",
    "trailing_avg_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sample_game():\n",
    "    env = envs[-1]\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset(adversary_plays_first=True)\n",
    "    obs = torch.from_numpy(obs).to(device).float().unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(obs, env)\n",
    "        new_obs, reward, done, metadata = env.step(np.array(action.cpu()))\n",
    "        new_obs = torch.from_numpy(new_obs).to(device).float().unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Move to the next state\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            print(env.state)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def run_training(num_episodes = 10000):\n",
    "    global envs\n",
    "    global ucb_action_picked_counter\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            adversary_randomness = ADVERSARY_RANDOMNESS * ADVERSARY_RANDOMNESS_DECAY ** (i_episode // 100)\n",
    "            envs.append(\n",
    "                MultipleSimulationRewardEnv(\n",
    "                    WeightedRandomSelectedAdversary((\n",
    "                        RandomAdversary(38), \n",
    "                        PolicyNetAdversary(target_net)), [adversary_randomness, 1-adversary_randomness])))\n",
    "            envs = envs[-10:]\n",
    "\n",
    "            play_sample_game()\n",
    "\n",
    "        env = random.choice(envs)\n",
    "        # Initialize the environment and state\n",
    "        obs = env.reset(adversary_plays_first=random.random() > 0.5)\n",
    "        obs = torch.from_numpy(obs).to(device).float().unsqueeze(0)\n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            action = select_action(obs, env)\n",
    "            new_obs, reward, done, metadata = env.step(np.array(action.cpu()))\n",
    "            new_obs = torch.from_numpy(new_obs).to(device).float().unsqueeze(0)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(obs, torch.Tensor(np.array([metadata[\"action\"]], dtype=np.int64)), new_obs, reward)\n",
    "\n",
    "            # update UCB action counter\n",
    "            ucb_action_picked_counter[metadata[\"action\"]] += 1\n",
    "\n",
    "            # Move to the next state\n",
    "            obs = new_obs\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss = optimize_model()\n",
    "            if done:\n",
    "                \n",
    "                trailing_avg_reward.append(reward[0].cpu().numpy())\n",
    "                if len(trailing_avg_reward) > trailing_avg_size:\n",
    "                    trailing_avg_reward.popleft()\n",
    "\n",
    "                print(f\"episode #{i_episode}, episode reward: {reward[0]}, avg_reward: {round(np.mean(trailing_avg_reward),2)}, episode length: {t+1}, loss: {loss}\")\n",
    "                # print(env.state)\n",
    "                break\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print('Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vul: None\n",
      "        S 92\n",
      "        H A3\n",
      "        D 32\n",
      "        C AKQJ764\n",
      "S 7543          S QJT\n",
      "H J76           H KT8542\n",
      "D K75           D T64\n",
      "C T92           C 5\n",
      "        S AK86\n",
      "        H Q9\n",
      "        D AQJ98\n",
      "        C 83\n",
      "\n",
      "West  North East  South\n",
      "      3S    4S    6D    \n",
      "7S    Pass  Pass  7N    \n",
      "Dbl   Pass  Pass  Pass  \n",
      "\n",
      "Declarer tricks: 11\n",
      "Score: N/S -300 E/W 300\n",
      "episode #0, episode reward: -3080.0, avg_reward: -3080.0, episode length: 6, loss: None\n",
      "episode #1, episode reward: -3400.0, avg_reward: -3240.0, episode length: 6, loss: None\n",
      "episode #2, episode reward: 1820.0, avg_reward: -1553.33, episode length: 8, loss: None\n",
      "episode #3, episode reward: 410.0, avg_reward: -1062.5, episode length: 5, loss: None\n",
      "episode #4, episode reward: -200.0, avg_reward: -890.0, episode length: 3, loss: None\n",
      "episode #5, episode reward: 350.0, avg_reward: -683.33, episode length: 6, loss: None\n",
      "episode #6, episode reward: -300.0, avg_reward: -628.57, episode length: 5, loss: None\n",
      "episode #7, episode reward: 200.0, avg_reward: -525.0, episode length: 3, loss: None\n",
      "episode #8, episode reward: 300.0, avg_reward: -433.33, episode length: 4, loss: None\n",
      "episode #9, episode reward: -230.0, avg_reward: -413.0, episode length: 4, loss: None\n",
      "episode #10, episode reward: 110.0, avg_reward: -365.45, episode length: 6, loss: None\n",
      "episode #11, episode reward: 370.0, avg_reward: -304.17, episode length: 4, loss: None\n",
      "episode #12, episode reward: -230.0, avg_reward: -298.46, episode length: 7, loss: None\n",
      "episode #13, episode reward: -240.0, avg_reward: -294.29, episode length: 3, loss: None\n",
      "episode #14, episode reward: -440.0, avg_reward: -304.0, episode length: 5, loss: None\n",
      "episode #15, episode reward: -260.0, avg_reward: -301.25, episode length: 4, loss: None\n",
      "episode #16, episode reward: 290.0, avg_reward: -266.47, episode length: 3, loss: None\n",
      "episode #17, episode reward: 490.0, avg_reward: -224.44, episode length: 4, loss: None\n",
      "episode #18, episode reward: -410.0, avg_reward: -234.21, episode length: 4, loss: None\n",
      "episode #19, episode reward: -120.0, avg_reward: -228.5, episode length: 4, loss: None\n",
      "episode #20, episode reward: 170.0, avg_reward: -209.52, episode length: 6, loss: None\n",
      "episode #21, episode reward: 280.0, avg_reward: -187.27, episode length: 7, loss: None\n",
      "episode #22, episode reward: -300.0, avg_reward: -192.17, episode length: 4, loss: None\n",
      "episode #23, episode reward: 180.0, avg_reward: -176.67, episode length: 4, loss: None\n",
      "episode #24, episode reward: -4960.0, avg_reward: -368.0, episode length: 6, loss: None\n",
      "episode #25, episode reward: -5680.0, avg_reward: -572.31, episode length: 9, loss: None\n",
      "episode #26, episode reward: 4360.0, avg_reward: -389.63, episode length: 6, loss: None\n",
      "episode #27, episode reward: -450.0, avg_reward: -391.79, episode length: 5, loss: None\n",
      "episode #28, episode reward: -4000.0, avg_reward: -516.21, episode length: 6, loss: None\n",
      "episode #29, episode reward: -420.0, avg_reward: -513.0, episode length: 5, loss: None\n",
      "episode #30, episode reward: -280.0, avg_reward: -505.48, episode length: 4, loss: None\n",
      "episode #31, episode reward: 460.0, avg_reward: -475.31, episode length: 4, loss: None\n",
      "episode #32, episode reward: -204.0, avg_reward: -467.09, episode length: 6, loss: None\n",
      "episode #33, episode reward: 210.0, avg_reward: -447.18, episode length: 8, loss: None\n",
      "episode #34, episode reward: -300.0, avg_reward: -442.97, episode length: 3, loss: None\n",
      "episode #35, episode reward: 520.0, avg_reward: -416.22, episode length: 9, loss: None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_429376/1042599396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# %lprun -f MultipleSimulationRewardEnv.generate_random_games run_training(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_429376/1356567746.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Initialize the environment and state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversary_plays_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_429376/864908763.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, adversary, adversary_plays_first)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madversary\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary_plays_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversary_plays_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_random_games\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary_plays_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_429376/864908763.py\u001b[0m in \u001b[0;36mgenerate_random_games\u001b[0;34m(self, n_games)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mdeal_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent_hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeal_order\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_training()\n",
    "# %lprun -f MultipleSimulationRewardEnv.generate_random_games run_training(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "with  cProfile.Profile() as pr:\n",
    "    run_training(10)\n",
    "pr.print_stats(sort='cumtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sample_game():\n",
    "    env = envs[-1]\n",
    "    # Initialize the environment and state\n",
    "    obs = env.reset(adversary_plays_first=False)\n",
    "    obs = torch.from_numpy(obs).to(device).float().unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(obs, env)\n",
    "        new_obs, reward, done, metadata = env.step(np.array(action.cpu()))\n",
    "        new_obs = torch.from_numpy(new_obs).to(device).float().unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Move to the next state\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            print(env.state)\n",
    "            break\n",
    "\n",
    "play_sample_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1559.,  525.,  416.,   88.,   88.,   88.,   88.,   88.,   88.,\n",
       "         88.,   87.,   87.,   87.,   87.,   87.,   87.,   88.,   87.,\n",
       "         87.,   88.,   87.,   88.,   88.,   95.,   95.,   95.,  105.,\n",
       "        119.,  126.,  138.,  139.,  149.,  189.,  199.,  268.,  323.,\n",
       "        401.,  533.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucb_action_picked_counter"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "833f2951c89d327e7221b1b21e18d4e857a38a5e0b54df03b5c8a3bb660b6f13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
