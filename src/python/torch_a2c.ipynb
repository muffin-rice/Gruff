{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import gym, gym.spaces\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from BridgeNetwork import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = pyspiel.load_game('bridge(use_double_dummy_result=true)')\n",
    "\n",
    "class BridgeEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BridgeEnv, self).__init__()    # Define action and observation space\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(38,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(571,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.generate_random_game()\n",
    "        return np.array(self.state.observation_tensor())\n",
    "\n",
    "    def step(self, action):\n",
    "        action = self.pick_action(action)\n",
    "\n",
    "        self.state.apply_action(action+52)\n",
    "        \n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action)\n",
    "        \n",
    "        # random opposing team\n",
    "        self.state.apply_action(random.choice(self.state.legal_actions()))\n",
    "\n",
    "        if self.state.current_phase() == 3:\n",
    "            return self.calculate_terminal_reward(action, invert=True)\n",
    "\n",
    "        return self.calculate_default_reward(action)\n",
    "\n",
    "    def calculate_default_reward(self, action):\n",
    "        obs = np.array(self.state.observation_tensor())\n",
    "        reward = 0\n",
    "        done = False\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def calculate_terminal_reward(self, action, invert=False):\n",
    "        obs = np.zeros(571)\n",
    "        reward = self.state.score_by_contract()[self.state.contract_index()]\n",
    "        if invert:\n",
    "            reward = -reward\n",
    "        done = True\n",
    "        return obs, reward, done, {\"action\": action}\n",
    "\n",
    "    def pick_action(self, action_vector):\n",
    "        action_vector = self.softmax(action_vector)\n",
    "        legal_action_mask = np.array(self.state.legal_actions_mask())[52:52+self.action_space.shape[0]]\n",
    "        masked_action_vector = action_vector*legal_action_mask / sum(action_vector*legal_action_mask)\n",
    "        action = np.random.choice(self.action_space.shape[0], p=masked_action_vector)\n",
    "\n",
    "        if action + 52 not in self.state.legal_actions():\n",
    "            print(action+52, self.state.legal_actions())\n",
    "            print(action_vector[:6])\n",
    "            print(legal_action_mask[:6])\n",
    "            print((action_vector*legal_action_mask)[:6])\n",
    "            print(masked_action_vector[:6])\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        y = np.exp(x - np.max(x))\n",
    "        f_x = y / np.sum(y)\n",
    "        return f_x\n",
    "\n",
    "    def generate_random_game(self): \n",
    "        state = GAME.new_initial_state()\n",
    "        # deal all 52 cards randomly\n",
    "        for i in np.random.choice(52, size=(52,), replace=False):\n",
    "            state.apply_action(i)\n",
    "        return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def get(self):\n",
    "        return self.memory\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 40\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "env = BridgeEnv()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.shape[0]\n",
    "n_observations = sum(env.observation_space.shape)\n",
    "\n",
    "actor_critic = BridgeActorCritic().to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr = LEARNING_RATE)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "trailing_avg_reward = deque()\n",
    "trailing_avg_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #0, episode reward: 5200, avg_reward: 907.0, episode length: 7\n",
      "episode #1, episode reward: 1000, avg_reward: 871.0, episode length: 7\n",
      "episode #2, episode reward: 400, avg_reward: 869.0, episode length: 7\n",
      "episode #3, episode reward: -2900, avg_reward: 838.0, episode length: 5\n",
      "episode #4, episode reward: 1600, avg_reward: 850.5, episode length: 7\n",
      "episode #5, episode reward: 550, avg_reward: 852.5, episode length: 5\n",
      "episode #6, episode reward: 550, avg_reward: 856.0, episode length: 4\n",
      "episode #7, episode reward: 250, avg_reward: 854.5, episode length: 3\n",
      "episode #8, episode reward: 2200, avg_reward: 866.5, episode length: 6\n",
      "episode #9, episode reward: 4000, avg_reward: 890.5, episode length: 7\n",
      "episode #10, episode reward: 500, avg_reward: 837.5, episode length: 4\n",
      "episode #11, episode reward: 100, avg_reward: 832.0, episode length: 4\n",
      "episode #12, episode reward: 4000, avg_reward: 869.5, episode length: 5\n",
      "episode #13, episode reward: -300, avg_reward: 832.5, episode length: 8\n",
      "episode #14, episode reward: 450, avg_reward: 815.0, episode length: 5\n",
      "episode #15, episode reward: 500, avg_reward: 816.0, episode length: 5\n",
      "episode #16, episode reward: -2300, avg_reward: 789.0, episode length: 4\n",
      "episode #17, episode reward: 300, avg_reward: 788.5, episode length: 2\n",
      "episode #18, episode reward: 350, avg_reward: 803.0, episode length: 7\n",
      "episode #19, episode reward: -2900, avg_reward: 752.0, episode length: 6\n",
      "episode #20, episode reward: 200, avg_reward: 771.0, episode length: 5\n",
      "episode #21, episode reward: 4000, avg_reward: 741.0, episode length: 9\n",
      "episode #22, episode reward: -2000, avg_reward: 747.0, episode length: 6\n",
      "episode #23, episode reward: 350, avg_reward: 779.5, episode length: 4\n",
      "episode #24, episode reward: 2200, avg_reward: 767.5, episode length: 4\n",
      "episode #25, episode reward: -1700, avg_reward: 748.5, episode length: 5\n",
      "episode #26, episode reward: 100, avg_reward: 747.0, episode length: 3\n",
      "episode #27, episode reward: 1600, avg_reward: 761.5, episode length: 7\n",
      "episode #28, episode reward: 400, avg_reward: 779.5, episode length: 5\n",
      "episode #29, episode reward: 3400, avg_reward: 809.5, episode length: 9\n",
      "episode #30, episode reward: 250, avg_reward: 796.0, episode length: 2\n",
      "episode #31, episode reward: 500, avg_reward: 800.0, episode length: 4\n",
      "episode #32, episode reward: -2900, avg_reward: 743.0, episode length: 5\n",
      "episode #33, episode reward: 1000, avg_reward: 750.0, episode length: 6\n",
      "episode #34, episode reward: 2800, avg_reward: 744.0, episode length: 6\n",
      "episode #35, episode reward: -2300, avg_reward: 681.0, episode length: 5\n",
      "episode #36, episode reward: 100, avg_reward: 654.0, episode length: 5\n",
      "episode #37, episode reward: 400, avg_reward: 655.0, episode length: 9\n",
      "episode #38, episode reward: 550, avg_reward: 632.5, episode length: 5\n",
      "episode #39, episode reward: 350, avg_reward: 650.0, episode length: 4\n",
      "episode #40, episode reward: 2800, avg_reward: 638.0, episode length: 5\n",
      "episode #41, episode reward: -2900, avg_reward: 629.0, episode length: 5\n",
      "episode #42, episode reward: 2200, avg_reward: 649.0, episode length: 5\n",
      "episode #43, episode reward: 200, avg_reward: 648.5, episode length: 5\n",
      "episode #44, episode reward: 2800, avg_reward: 693.5, episode length: 8\n",
      "episode #45, episode reward: 150, avg_reward: 689.0, episode length: 5\n",
      "episode #46, episode reward: 4000, avg_reward: 677.0, episode length: 6\n",
      "episode #47, episode reward: 250, avg_reward: 690.5, episode length: 4\n",
      "episode #48, episode reward: 500, avg_reward: 693.0, episode length: 8\n",
      "episode #49, episode reward: -2600, avg_reward: 661.0, episode length: 4\n",
      "episode #50, episode reward: -1700, avg_reward: 638.0, episode length: 5\n",
      "episode #51, episode reward: 2200, avg_reward: 657.5, episode length: 5\n",
      "episode #52, episode reward: 4000, avg_reward: 657.5, episode length: 6\n",
      "episode #53, episode reward: -2900, avg_reward: 651.5, episode length: 4\n",
      "episode #54, episode reward: -2000, avg_reward: 609.5, episode length: 7\n",
      "episode #55, episode reward: 550, avg_reward: 587.0, episode length: 4\n",
      "episode #56, episode reward: 1000, avg_reward: 623.0, episode length: 4\n",
      "episode #57, episode reward: 6400, avg_reward: 684.0, episode length: 5\n",
      "episode #58, episode reward: 400, avg_reward: 682.5, episode length: 4\n",
      "episode #59, episode reward: 300, avg_reward: 682.5, episode length: 6\n",
      "episode #60, episode reward: -2300, avg_reward: 601.5, episode length: 6\n",
      "episode #61, episode reward: 4600, avg_reward: 589.5, episode length: 8\n",
      "episode #62, episode reward: 2200, avg_reward: 607.5, episode length: 7\n",
      "episode #63, episode reward: -800, avg_reward: 619.5, episode length: 3\n",
      "episode #64, episode reward: 5800, avg_reward: 673.5, episode length: 8\n",
      "episode #65, episode reward: 500, avg_reward: 668.5, episode length: 4\n",
      "episode #66, episode reward: 200, avg_reward: 664.5, episode length: 3\n",
      "episode #67, episode reward: -1520, avg_reward: 646.8, episode length: 4\n",
      "episode #68, episode reward: 250, avg_reward: 646.8, episode length: 6\n",
      "episode #69, episode reward: 200, avg_reward: 646.3, episode length: 3\n",
      "episode #70, episode reward: 450, avg_reward: 646.3, episode length: 3\n",
      "episode #71, episode reward: 2800, avg_reward: 670.8, episode length: 8\n",
      "episode #72, episode reward: -2000, avg_reward: 647.8, episode length: 5\n",
      "episode #73, episode reward: 300, avg_reward: 646.8, episode length: 4\n",
      "episode #74, episode reward: 300, avg_reward: 672.8, episode length: 3\n",
      "episode #75, episode reward: -2300, avg_reward: 654.8, episode length: 5\n",
      "episode #76, episode reward: 550, avg_reward: 657.8, episode length: 7\n",
      "episode #77, episode reward: -800, avg_reward: 603.8, episode length: 8\n",
      "episode #78, episode reward: 500, avg_reward: 631.8, episode length: 2\n",
      "episode #79, episode reward: 400, avg_reward: 631.8, episode length: 4\n",
      "episode #80, episode reward: 1600, avg_reward: 643.8, episode length: 6\n",
      "episode #81, episode reward: 2800, avg_reward: 619.8, episode length: 5\n",
      "episode #82, episode reward: 450, avg_reward: 621.8, episode length: 5\n",
      "episode #83, episode reward: 2200, avg_reward: 642.3, episode length: 7\n",
      "episode #84, episode reward: 3400, avg_reward: 673.3, episode length: 8\n",
      "episode #85, episode reward: 400, avg_reward: 697.3, episode length: 5\n",
      "episode #86, episode reward: 1600, avg_reward: 733.3, episode length: 6\n",
      "episode #87, episode reward: -500, avg_reward: 700.3, episode length: 5\n",
      "episode #88, episode reward: -1520, avg_reward: 651.1, episode length: 4\n",
      "episode #89, episode reward: 350, avg_reward: 651.6, episode length: 4\n",
      "episode #90, episode reward: 4000, avg_reward: 657.6, episode length: 5\n",
      "episode #91, episode reward: -2000, avg_reward: 615.6, episode length: 7\n",
      "episode #92, episode reward: 450, avg_reward: 643.1, episode length: 7\n",
      "episode #93, episode reward: 5800, avg_reward: 696.1, episode length: 4\n",
      "episode #94, episode reward: 4600, avg_reward: 740.1, episode length: 6\n",
      "episode #95, episode reward: -1700, avg_reward: 718.1, episode length: 4\n",
      "episode #96, episode reward: -3200, avg_reward: 685.6, episode length: 5\n",
      "episode #97, episode reward: 500, avg_reward: 687.6, episode length: 3\n",
      "episode #98, episode reward: 1000, avg_reward: 708.6, episode length: 9\n",
      "episode #99, episode reward: 350, avg_reward: 684.1, episode length: 4\n",
      "episode #100, episode reward: 300, avg_reward: 635.1, episode length: 4\n",
      "episode #101, episode reward: 4000, avg_reward: 665.1, episode length: 6\n",
      "episode #102, episode reward: 350, avg_reward: 664.6, episode length: 6\n",
      "episode #103, episode reward: 400, avg_reward: 697.6, episode length: 2\n",
      "episode #104, episode reward: 1000, avg_reward: 691.6, episode length: 7\n",
      "episode #105, episode reward: 3400, avg_reward: 720.1, episode length: 6\n",
      "episode #106, episode reward: -2300, avg_reward: 691.6, episode length: 8\n",
      "episode #107, episode reward: 4000, avg_reward: 729.1, episode length: 8\n",
      "episode #108, episode reward: 400, avg_reward: 711.1, episode length: 5\n",
      "episode #109, episode reward: 3400, avg_reward: 705.1, episode length: 10\n",
      "episode #110, episode reward: 150, avg_reward: 701.6, episode length: 3\n",
      "episode #111, episode reward: 250, avg_reward: 703.1, episode length: 6\n",
      "episode #112, episode reward: -2000, avg_reward: 643.1, episode length: 6\n",
      "episode #113, episode reward: -1400, avg_reward: 632.1, episode length: 6\n",
      "episode #114, episode reward: -1700, avg_reward: 610.6, episode length: 5\n",
      "episode #115, episode reward: -500, avg_reward: 600.6, episode length: 7\n",
      "episode #116, episode reward: 550, avg_reward: 629.1, episode length: 5\n",
      "episode #117, episode reward: 4000, avg_reward: 666.1, episode length: 6\n",
      "episode #118, episode reward: -3200, avg_reward: 630.6, episode length: 6\n",
      "episode #119, episode reward: 3400, avg_reward: 693.6, episode length: 6\n",
      "episode #120, episode reward: 1600, avg_reward: 707.6, episode length: 8\n",
      "episode #121, episode reward: 2200, avg_reward: 689.6, episode length: 5\n",
      "episode #122, episode reward: 3400, avg_reward: 743.6, episode length: 5\n",
      "episode #123, episode reward: 600, avg_reward: 746.1, episode length: 4\n",
      "episode #124, episode reward: -1400, avg_reward: 710.1, episode length: 6\n",
      "episode #125, episode reward: 350, avg_reward: 730.6, episode length: 4\n",
      "episode #126, episode reward: 2800, avg_reward: 757.6, episode length: 8\n",
      "episode #127, episode reward: -1700, avg_reward: 724.6, episode length: 6\n",
      "episode #128, episode reward: 2800, avg_reward: 748.6, episode length: 7\n",
      "episode #129, episode reward: 300, avg_reward: 717.6, episode length: 4\n",
      "episode #130, episode reward: -3500, avg_reward: 680.1, episode length: 3\n",
      "episode #131, episode reward: 5200, avg_reward: 727.1, episode length: 8\n",
      "episode #132, episode reward: 200, avg_reward: 758.1, episode length: 3\n",
      "episode #133, episode reward: 450, avg_reward: 752.6, episode length: 5\n",
      "episode #134, episode reward: 350, avg_reward: 728.1, episode length: 4\n",
      "episode #135, episode reward: 450, avg_reward: 755.6, episode length: 3\n",
      "episode #136, episode reward: 450, avg_reward: 759.1, episode length: 4\n",
      "episode #137, episode reward: 1000, avg_reward: 765.1, episode length: 6\n",
      "episode #138, episode reward: 100, avg_reward: 760.6, episode length: 5\n",
      "episode #139, episode reward: 4000, avg_reward: 797.1, episode length: 11\n",
      "episode #140, episode reward: -1400, avg_reward: 755.1, episode length: 3\n",
      "episode #141, episode reward: 150, avg_reward: 785.6, episode length: 3\n",
      "episode #142, episode reward: 250, avg_reward: 766.1, episode length: 5\n",
      "episode #143, episode reward: 350, avg_reward: 767.6, episode length: 3\n",
      "episode #144, episode reward: -3200, avg_reward: 707.6, episode length: 7\n",
      "episode #145, episode reward: 5200, avg_reward: 758.1, episode length: 6\n",
      "episode #146, episode reward: -1700, avg_reward: 701.1, episode length: 4\n",
      "episode #147, episode reward: 150, avg_reward: 700.1, episode length: 4\n",
      "episode #148, episode reward: 600, avg_reward: 701.1, episode length: 4\n",
      "episode #149, episode reward: -2000, avg_reward: 707.1, episode length: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_95554/151646388.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Initialize the environment and state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_95554/84800770.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_random_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_95554/84800770.py\u001b[0m in \u001b[0;36mgenerate_random_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# deal all 52 cards randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).to(device).float().unsqueeze(0)\n",
    "    for t in count():\n",
    "        value, policy_dist = actor_critic.forward(state)\n",
    "\n",
    "        value = value.detach().numpy()[0,0]\n",
    "        dist = policy_dist.detach().squeeze().numpy()\n",
    "\n",
    "        new_state, reward, done, metadata = env.step(dist)\n",
    "        new_state = torch.from_numpy(new_state).to(device).float().unsqueeze(0)\n",
    "\n",
    "        action = metadata[\"action\"]\n",
    "        log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "\n",
    "            Qval, _ = actor_critic.forward(new_state)\n",
    "\n",
    "            trailing_avg_reward.append(reward)\n",
    "            if len(trailing_avg_reward) > trailing_avg_size:\n",
    "                trailing_avg_reward.popleft()\n",
    "\n",
    "            \n",
    "            print(f\"episode #{i_episode}, episode reward: {reward}, avg_reward: {round(np.mean(trailing_avg_reward),2)}, episode length: {t+1}\")\n",
    "\n",
    "\n",
    "            #print(env.state)\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t]/100 + GAMMA * Qval\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    values = torch.FloatTensor(values) # values calculated by Critic\n",
    "    Qvals = torch.FloatTensor(Qvals) # real values (calculated by sum of episode reward * discount factor)\n",
    "    log_probs = torch.stack(log_probs) # log probability of each move in the episode\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss =  (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print('Complete')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "833f2951c89d327e7221b1b21e18d4e857a38a5e0b54df03b5c8a3bb660b6f13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
